{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import  SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Objectives \n",
    "\n",
    "By the end of this lecture students should be able to, \n",
    "\n",
    "1. Define NLP, how it is used, and explain NLP's challenges.\n",
    "2. Describe the NLP workflow.\n",
    "3. Execute NLP workflow with python libraries \n",
    "\n",
    "### Requirements\n",
    "\n",
    "You need to install the `nltk` module:\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "```\n",
    "\n",
    "This module will need corporas that you need to download. This can take a very long time, for simplicity here's the minimal corporas for this lecture. In a terminal, open `ipython` , import nltk, and type:\n",
    "\n",
    "```\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "<!--\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLP\n",
    "\n",
    "### What is Natural Language Processing (NLP)?\n",
    "\n",
    "The machine learning models we create are dependent on numerical data. However text, which is non-numerical, is\n",
    "diverse and plentiful. So, how can we get computers to understand the languages of humans? In NLP we are trying to program computers to process and analyze large amounts of natural language data.  To do this we have to use various techniques to convert words or language into a numerical representation so that machine learning algorithms can interpret language.\n",
    "\n",
    "<!-- ![](https://miro.medium.com/max/1532/1*JFSVBcw7GDr7v-8hcqmPDA.png) -->\n",
    "\n",
    "<img src='http://hoctructuyen123.net/wp-content/uploads/2019/07/Untitled.png' width=500 >\n",
    "\n",
    "### Example Applications of NLP \n",
    "\n",
    "NLP is a huge field with many exciting applications.  Here are just a few: \n",
    "- **Gmail:** Classifying spam emails \n",
    "- **Chat bots:** Ability to interact with a chat bot on websites\n",
    "- **Google:** Automatically group news articles by topic \n",
    "- **Amazon Reviews:** Is a review on amazon positive, negative or neutral? \n",
    "\n",
    "<img src='https://www.pantechsolutions.net/blog/wp-content/uploads/2019/05/NLP.png' width=500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### It is difficult to understand human language! \n",
    "\n",
    "What is the meaning of the following sentence? \n",
    "\n",
    "> I made him duck. \n",
    "\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "**There is a lot of ambiguity in language!**\n",
    "<table>\n",
    "   <td> <img src='https://bloximages.chicago2.vip.townnews.com/tucson.com/content/tncms/assets/v3/editorial/2/7b/27bbe6b5-3dc0-58ef-9f6e-ed4c88dbf224/4fb3c6fd2fc1a.image.jpg' width=400> \n",
    "   <td> <img src='https://www.nutritionadvance.com/wp-content/uploads/2018/11/duck-meat-is-it-a-healthy-choice.jpg' width=300> \n",
    "   <td> <img src='https://image.invaluable.com/housePhotos/PeachtreeandBennett/26/559426/H4390-L67817296.jpg' width=300> \n",
    "   <td> <img src ='https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Grave_eend_maasmuur.jpg/1200px-Grave_eend_maasmuur.jpg' width=300> \n",
    "</table>\n",
    "\n",
    "This problem of determining which sense was meant by a specific word is  formally known as **word sense disambiguation**.\n",
    "\n",
    "<img src='images/nlp_python_cartoon.png' width=600>\n",
    "\n",
    "Often times we want to get at the semantic meaning of langugage, but unfortunately language is set up where this is pretty difficult.  This is something to be aware of when using NLP.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### NLP Terminology \n",
    "\n",
    "Before we dive into the conversion of natural language to numerical vectors, we first need to introudce terminology commonly used in NLP.  \n",
    "\n",
    "| Term      | Meaning     | \n",
    "| :------------- | :----------: | \n",
    "|  *Corpus* | Collection of documents (collection of articles)   | \n",
    "|  *Document* | A single document (a tweet, an email, an article) | \n",
    "|  *Vocabulary*  | Set of words in your corpus, or maybe the entire English dictionary | \n",
    "|  *Bag of Words*  | Vector representation of words in a document | \n",
    "|  *Token*  | Single word | \n",
    "|  *Stop Words*  | Common ignored words because not useful in distinguishing text | \n",
    "|  *Vectorizing*  | Converting text into a bag-of-words | \n",
    "|  *n-gram*  | How many words constitute a linguistic unit in the analysis (*i.e* boy vs little boy)  |\n",
    "|  *stemming*  | Cuts off beginning or end of words to get at root meaning (*i.e* studying --> study)  |\n",
    "|  *lemmatizing*  | Similar to stemming but gets at the root meaning of the word (*i.e* studied --> study)  |\n",
    "\n",
    "\n",
    "I will expand on this language throughout the lecture.  Let's start with how we convert language into a vector: **bag of words**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words \n",
    "\n",
    "One of the most common ways to represent text numerically is using Bag of Words (BoW).\n",
    "\n",
    "![](images/BOW.png)\n",
    "\n",
    "<!--<img src='images/bag_words1.png' width=800 />-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline \n",
    "\n",
    "## NLP tools\n",
    "\n",
    "Throughout this lecture we will be introducing several tools which can be used to help with the NLP pipeline.  These tools include:\n",
    "\n",
    "- sklearn \n",
    "- nltk\n",
    "- spacey\n",
    "\n",
    "I would like to point out that each of these tools have an important role to play.  \n",
    "\n",
    "- sklearn: great for a first pass; easy to implement but you have the least amount of control \n",
    "- nltk: solid nlp library; great nlp learning tool \n",
    "- spacey: good for production environments\n",
    "\n",
    "With theses notes in mind, I am going to start by using nltk and implement the entire NLP pipeline. That being said, please be aware of the other tools you have when working on NLP projects. \n",
    "\n",
    "## Pipeline\n",
    "![](images/pipeline-walkthrough1.png)\n",
    "\n",
    "Below is a to do list when converting text into vector form: \n",
    "\n",
    "**Clean text and Create a Bag of Words (BoW)**\n",
    ">1. Lowercase the text\n",
    "2. Tokenize \n",
    "3. Strip out punctuation or undesirable text\n",
    "4. Remove Stopwords \n",
    "5. Stemming or Lemmatizing\n",
    "6. Compute N-Grams\n",
    "7. Use this to create BoW\n",
    "\n",
    "**Vectorize BoW**\n",
    ">8. Term Frequencies\n",
    "9. Document Frequencies\n",
    "10. TF-IDF\n",
    "11. Normalize vectors\n",
    "\n",
    "Let's go through what each of these steps are and how to do them in python with the following corpus of comments about data science...\n",
    "\n",
    "Below we define some text that we are going to vectorize in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    'I ~love~ ^love^ *love* to Study DATA and SCience...',\n",
    "    '     a data science üòÄü§Øüëç is a üî•   career field.',\n",
    "    'Janet is studying the data science @ GALVANIZE!!!. Janet LOVES data aNd scieNCE.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters \n",
    "\n",
    "Next, special characters are removed by only allowing ASCII characters.  \n",
    "<br>\n",
    "<details>\n",
    "   <summary>\n",
    "       <b>Click here</b> for a few things you may want to know:\n",
    "    </summary>\n",
    "<b>unicode:</b>\n",
    "<ul>\n",
    "<li>an information technology standard for the consistent encoding, representation, and handling of text expressed in most of the world's writing systems. -- wiki\n",
    "<li>includes over 140,000 characters in various languages and emojis \n",
    "    </ul>\n",
    "<b>ascii:</b> \n",
    "<ul>\n",
    "<li> American Standard Code for Information Interchange, is a character encoding standard for electronic communication. -- wiki \n",
    "<li> limited to english characters, basic punctunation, numbers (128 characters)\n",
    "</ul>\n",
    "    \n",
    "Below we did the following: \n",
    "<ul>\n",
    "<li> normalized the text following unicodes 'NFKD' normalization form\n",
    "<li> encode text as ASCII and drop characters that could not be included (here we are sticking with the english alphabet and basic punctuation and droping anything else)\n",
    "<li> then we decode it back to unicode using the utf8 (8-bit representation of unicode characters)\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     a data science  is a    career field.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKD', docs[1]).encode('ASCII', 'ignore').decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by removing emojis and other special characters\n",
    "for i,doc in enumerate(docs):\n",
    "    docs[i] = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I ~love~ ^love^ *love* to Study DATA and SCience...',\n",
       " '     a data science  is a    career field.',\n",
       " 'Janet is studying the data science @ GALVANIZE!!!. Janet LOVES data aNd scieNCE.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text and Create a BoWs\n",
    "\n",
    "### Lowercase the text \n",
    "\n",
    "This one is pretty straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i ~love~ ^love^ *love* to study data and science...',\n",
       " '     a data science  is a    career field.',\n",
       " 'janet is studying the data science @ galvanize!!!. janet loves data and science.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    docs[i] = doc.lower()  \n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "Convert text into list of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  '~love~',\n",
       "  '^love^',\n",
       "  '*',\n",
       "  'love',\n",
       "  '*',\n",
       "  'to',\n",
       "  'study',\n",
       "  'data',\n",
       "  'and',\n",
       "  'science',\n",
       "  '...'],\n",
       " ['a', 'data', 'science', 'is', 'a', 'career', 'field', '.'],\n",
       " ['janet',\n",
       "  'is',\n",
       "  'studying',\n",
       "  'the',\n",
       "  'data',\n",
       "  'science',\n",
       "  '@',\n",
       "  'galvanize',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!',\n",
       "  '.',\n",
       "  'janet',\n",
       "  'loves',\n",
       "  'data',\n",
       "  'and',\n",
       "  'science',\n",
       "  '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    docs[i] = word_tokenize(doc) # method in nltk\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip out punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = string.punctuation\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  '~love~',\n",
       "  '^love^',\n",
       "  'love',\n",
       "  'to',\n",
       "  'study',\n",
       "  'data',\n",
       "  'and',\n",
       "  'science',\n",
       "  '...'],\n",
       " ['a', 'data', 'science', 'is', 'a', 'career', 'field'],\n",
       " ['janet',\n",
       "  'is',\n",
       "  'studying',\n",
       "  'the',\n",
       "  'data',\n",
       "  'science',\n",
       "  'galvanize',\n",
       "  'janet',\n",
       "  'loves',\n",
       "  'data',\n",
       "  'and',\n",
       "  'science']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    docs[i] = [word for word in doc if not word in pt]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so this didn't remove all punctuations lets do a bit more to eliminate un-needed punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expression to remove additional punctuation\n",
    "for i,doc in enumerate(docs):\n",
    "    for j,word in enumerate(doc):\n",
    "        docs[i][j] = re.sub(r'[~*.^]*', '', doc[j])  \n",
    "\n",
    "# r is raw string -- it is treated as it appears\n",
    "# Without the r, backslashes are treated as escape characters. \n",
    "# With the r, backslashes are treated as literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'love', 'love', 'to', 'study', 'data', 'and', 'science', ''],\n",
       " ['a', 'data', 'science', 'is', 'a', 'career', 'field'],\n",
       " ['janet',\n",
       "  'is',\n",
       "  'studying',\n",
       "  'the',\n",
       "  'data',\n",
       "  'science',\n",
       "  'galvanize',\n",
       "  'janet',\n",
       "  'loves',\n",
       "  'data',\n",
       "  'and',\n",
       "  'science']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    if '' in doc:\n",
    "        doc.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'love', 'love', 'to', 'study', 'data', 'and', 'science'],\n",
       " ['a', 'data', 'science', 'is', 'a', 'career', 'field'],\n",
       " ['janet',\n",
       "  'is',\n",
       "  'studying',\n",
       "  'the',\n",
       "  'data',\n",
       "  'science',\n",
       "  'galvanize',\n",
       "  'janet',\n",
       "  'loves',\n",
       "  'data',\n",
       "  'and',\n",
       "  'science']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords \n",
    "\n",
    "Sometimes, some extremely common words which would appear to be of little value in understanding the content of language are excluded from the vocabulary entirely. These words are called **stop words**.\n",
    "\n",
    "**Examples:**\n",
    "1. the\n",
    "2. is\n",
    "3. to\n",
    "\n",
    "NLTK comes with a default list of stop words you can remove from your corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english') # stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'love', 'love', 'study', 'data', 'science'],\n",
       " ['data', 'science', 'career', 'field'],\n",
       " ['janet',\n",
       "  'studying',\n",
       "  'data',\n",
       "  'science',\n",
       "  'galvanize',\n",
       "  'janet',\n",
       "  'loves',\n",
       "  'data',\n",
       "  'science']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    docs[i] = [token for token in doc if token not in sw]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "I have used a nltk's stop words.  However, you can feed in your own list of stopwords! Customizing this process to your problem is an important part of the NLP process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming or Lemmatizing\n",
    "\n",
    "**Stemming:**<br>\n",
    "- Removes suffixes \n",
    "    - speaking -> speak\n",
    "    - actually -> actual\n",
    "- Does it without context \n",
    "- Some times can produce weird words \n",
    "- Pretty fast \n",
    "\n",
    "**Lemmatizing** \n",
    "- Replace word with root word or lemma \n",
    "   - better -> good \n",
    "   - funny -> fun \n",
    "- Slower \n",
    "- Can do a better job and getting at root meaning of word\n",
    "\n",
    "![](https://miro.medium.com/max/942/1*1bZumlAPUQwfEvQXbmWI_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with NLTK \n",
    "\n",
    "There are several types of stemming algorithms.  Here are three commonly used Stemming algorithms:\n",
    "\n",
    "1. Porter: Commonly used algorithm and one of the least aggressive (Less likely to remove letters from words)\n",
    "2. Snowball: An improved version of Porter that is move efficient.\n",
    "3. Lancaster: An aggressive and efficient method; however, it is difficult to interpret the root meaning of words. \n",
    "\n",
    "Overall, I would suggest Snowball as it is both efficient and interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'love', 'love', 'studi', 'data', 'scienc'],\n",
       " ['data', 'scienc', 'career', 'field'],\n",
       " ['janet',\n",
       "  'studi',\n",
       "  'data',\n",
       "  'scienc',\n",
       "  'galvan',\n",
       "  'janet',\n",
       "  'love',\n",
       "  'data',\n",
       "  'scienc']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1 = []\n",
    "for i,doc in enumerate(docs):\n",
    "    docs1.append([stemmer.stem(token) for token in doc])\n",
    "docs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing with NLTK\n",
    "\n",
    "First let's look at how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studies', 'studying', 'cries', 'cry']\n",
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = word_tokenize(text)\n",
    "print(tokenization)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now lets use it to transform our data science commments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'love', 'love', 'study', 'data', 'science'],\n",
       " ['data', 'science', 'career', 'field'],\n",
       " ['janet',\n",
       "  'studying',\n",
       "  'data',\n",
       "  'science',\n",
       "  'galvanize',\n",
       "  'janet',\n",
       "  'love',\n",
       "  'data',\n",
       "  'science']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    docs[i] = [lemmatizer.lemmatize(token) for token in doc]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute N-Grams\n",
    "\n",
    "N-grams of texts are extensively used in text mining and natural language processing tasks. They are a set of co-occurring words which are found via a sliding window.  \n",
    "\n",
    "- Pro: They are helpful when pairs of words (or >2) give insight to the content of text\n",
    "- Cons:  They increase the dimensionality of an already very high dimensional problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'like'), ('like', 'data'), ('data', 'science')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(['I','like','data','science'],2)) # n-grams is a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,doc in enumerate(docs):\n",
    "    ng = list(map(lambda tup: '-'.join(tup), ngrams(doc, 2)))\n",
    "    docs[i] = doc + ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love',\n",
       "  'love',\n",
       "  'love',\n",
       "  'study',\n",
       "  'data',\n",
       "  'science',\n",
       "  'love-love',\n",
       "  'love-love',\n",
       "  'love-study',\n",
       "  'study-data',\n",
       "  'data-science'],\n",
       " ['data',\n",
       "  'science',\n",
       "  'career',\n",
       "  'field',\n",
       "  'data-science',\n",
       "  'science-career',\n",
       "  'career-field'],\n",
       " ['janet',\n",
       "  'studying',\n",
       "  'data',\n",
       "  'science',\n",
       "  'galvanize',\n",
       "  'janet',\n",
       "  'love',\n",
       "  'data',\n",
       "  'science',\n",
       "  'janet-studying',\n",
       "  'studying-data',\n",
       "  'data-science',\n",
       "  'science-galvanize',\n",
       "  'galvanize-janet',\n",
       "  'janet-love',\n",
       "  'love-data',\n",
       "  'data-science']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary and initializing the BoW matrix\n",
    "vocabulary = set()\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        vocabulary.add(token)\n",
    "\n",
    "vocabulary_lookup = {word: i for i, word in enumerate(vocabulary)}\n",
    "matrix = np.zeros((len(docs), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'studying-data': 0,\n",
       " 'janet-love': 1,\n",
       " 'science-galvanize': 2,\n",
       " 'career-field': 3,\n",
       " 'science-career': 4,\n",
       " 'study-data': 5,\n",
       " 'love': 6,\n",
       " 'love-love': 7,\n",
       " 'galvanize': 8,\n",
       " 'career': 9,\n",
       " 'love-data': 10,\n",
       " 'field': 11,\n",
       " 'love-study': 12,\n",
       " 'studying': 13,\n",
       " 'data': 14,\n",
       " 'data-science': 15,\n",
       " 'janet': 16,\n",
       " 'galvanize-janet': 17,\n",
       " 'science': 18,\n",
       " 'janet-studying': 19,\n",
       " 'study': 20}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_lookup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bag of words\n",
    "for doc_id, doc in enumerate(docs):\n",
    "    for token in doc:\n",
    "        word_id = vocabulary_lookup[token]\n",
    "        matrix[doc_id][word_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studying-data</th>\n",
       "      <th>janet-love</th>\n",
       "      <th>science-galvanize</th>\n",
       "      <th>career-field</th>\n",
       "      <th>science-career</th>\n",
       "      <th>study-data</th>\n",
       "      <th>love</th>\n",
       "      <th>love-love</th>\n",
       "      <th>galvanize</th>\n",
       "      <th>career</th>\n",
       "      <th>...</th>\n",
       "      <th>field</th>\n",
       "      <th>love-study</th>\n",
       "      <th>studying</th>\n",
       "      <th>data</th>\n",
       "      <th>data-science</th>\n",
       "      <th>janet</th>\n",
       "      <th>galvanize-janet</th>\n",
       "      <th>science</th>\n",
       "      <th>janet-studying</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   studying-data  janet-love  science-galvanize  career-field  science-career  \\\n",
       "0              0           0                  0             0               0   \n",
       "1              0           0                  0             1               1   \n",
       "2              1           1                  1             0               0   \n",
       "\n",
       "   study-data  love  love-love  galvanize  career  ...  field  love-study  \\\n",
       "0           1     3          2          0       0  ...      0           1   \n",
       "1           0     0          0          0       1  ...      1           0   \n",
       "2           0     1          0          1       0  ...      0           0   \n",
       "\n",
       "   studying  data  data-science  janet  galvanize-janet  science  \\\n",
       "0         0     1             1      0                0        1   \n",
       "1         0     1             1      0                0        1   \n",
       "2         1     2             2      2                1        2   \n",
       "\n",
       "   janet-studying  study  \n",
       "0               0      1  \n",
       "1               0      0  \n",
       "2               1      0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizing the bag of words\n",
    "columns = sorted(vocabulary_lookup, key=lambda key: vocabulary_lookup[key])\n",
    "df = pd.DataFrame(matrix.astype('int'), columns=columns); df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review \n",
    "\n",
    "What were the steps we took to convert raw text into the BoW?\n",
    "\n",
    "## Vectorize the BoWs\n",
    "\n",
    "### Term Frequencies\n",
    "\n",
    "The percentage of number of times a term occurs in a specific document:\n",
    "\n",
    "$tf(term,document) = \\frac{\\# \\ of \\ times \\ a \\ term \\ appears \\ in \\ a \\ document}{\\#\\ of\\ terms\\ in\\ the\\ document}$\n",
    "\n",
    "Note: You can use the L2 norm as well for the denominator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studying-data</th>\n",
       "      <th>janet-love</th>\n",
       "      <th>science-galvanize</th>\n",
       "      <th>career-field</th>\n",
       "      <th>science-career</th>\n",
       "      <th>study-data</th>\n",
       "      <th>love</th>\n",
       "      <th>love-love</th>\n",
       "      <th>galvanize</th>\n",
       "      <th>career</th>\n",
       "      <th>...</th>\n",
       "      <th>field</th>\n",
       "      <th>love-study</th>\n",
       "      <th>studying</th>\n",
       "      <th>data</th>\n",
       "      <th>data-science</th>\n",
       "      <th>janet</th>\n",
       "      <th>galvanize-janet</th>\n",
       "      <th>science</th>\n",
       "      <th>janet-studying</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   studying-data  janet-love  science-galvanize  career-field  science-career  \\\n",
       "0           0.00        0.00               0.00          0.00            0.00   \n",
       "1           0.00        0.00               0.00          0.14            0.14   \n",
       "2           0.06        0.06               0.06          0.00            0.00   \n",
       "\n",
       "   study-data  love  love-love  galvanize  career  ...  field  love-study  \\\n",
       "0        0.09  0.27       0.18       0.00    0.00  ...   0.00        0.09   \n",
       "1        0.00  0.00       0.00       0.00    0.14  ...   0.14        0.00   \n",
       "2        0.00  0.06       0.00       0.06    0.00  ...   0.00        0.00   \n",
       "\n",
       "   studying  data  data-science  janet  galvanize-janet  science  \\\n",
       "0      0.00  0.09          0.09   0.00             0.00     0.09   \n",
       "1      0.00  0.14          0.14   0.00             0.00     0.14   \n",
       "2      0.06  0.12          0.12   0.12             0.06     0.12   \n",
       "\n",
       "   janet-studying  study  \n",
       "0            0.00   0.09  \n",
       "1            0.00   0.00  \n",
       "2            0.06   0.00  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dividing each document vector by the sum of its counts, producing a term frequency vector\n",
    "tf = df / df.sum(axis=1).values.reshape(-1, 1); tf.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the problems if we just used the term frequency as our numerical representation of text?\n",
    "\n",
    "\n",
    "\n",
    "**Example:** Consider the three tweets: \n",
    "\n",
    "- Data science rocks\n",
    "- Rock climbing data rocks\n",
    "- Science rocks\n",
    "\n",
    "And we put these documents into a BoWs:\n",
    "\n",
    "| Data        | Science     | Rock      | Climb |\n",
    "| ----------- | ----------- | ----------| ----------- |\n",
    "| 1.          | 1.          |1          | 0 |\n",
    "| 1.          | 0.          |2          | 1  |\n",
    "| 0.          | 1.          |1          | 0 |\n",
    "\n",
    "Here the Term Frequency for Rock would be high in each case.  However, the words which are more rare are actually more informative into how the tweets are different.\n",
    "\n",
    "### Document Frequency (DF)\n",
    "\n",
    "The ratio of documents with word, w, divided by number of all documents\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "$idf(term,corpus) = \\log{\\frac{1}{df(term,corpus)}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1 , 1.1 , 1.1 , 1.1 , 1.1 , 1.1 , 0.41, 1.1 , 1.1 , 1.1 , 1.1 , 1.1 , 1.1 , 1.1 , 0.  ,\n",
       "       0.  , 1.1 , 1.1 , 0.  , 1.1 , 1.1 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing the inverse document frequency\n",
    "idf = np.log(matrix.shape[0] / np.sum(matrix > 0, axis=0)); idf.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector. \n",
    "\n",
    "tf-idf $ = tf(term,document) * idf(term,corpus)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studying-data</th>\n",
       "      <th>janet-love</th>\n",
       "      <th>science-galvanize</th>\n",
       "      <th>career-field</th>\n",
       "      <th>science-career</th>\n",
       "      <th>study-data</th>\n",
       "      <th>love</th>\n",
       "      <th>love-love</th>\n",
       "      <th>galvanize</th>\n",
       "      <th>career</th>\n",
       "      <th>...</th>\n",
       "      <th>field</th>\n",
       "      <th>love-study</th>\n",
       "      <th>studying</th>\n",
       "      <th>data</th>\n",
       "      <th>data-science</th>\n",
       "      <th>janet</th>\n",
       "      <th>galvanize-janet</th>\n",
       "      <th>science</th>\n",
       "      <th>janet-studying</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   studying-data  janet-love  science-galvanize  career-field  science-career  \\\n",
       "0           0.00        0.00               0.00          0.00            0.00   \n",
       "1           0.00        0.00               0.00          0.16            0.16   \n",
       "2           0.06        0.06               0.06          0.00            0.00   \n",
       "\n",
       "   study-data  love  love-love  galvanize  career  ...  field  love-study  \\\n",
       "0         0.1  0.11        0.2       0.00    0.00  ...   0.00         0.1   \n",
       "1         0.0  0.00        0.0       0.00    0.16  ...   0.16         0.0   \n",
       "2         0.0  0.02        0.0       0.06    0.00  ...   0.00         0.0   \n",
       "\n",
       "   studying  data  data-science  janet  galvanize-janet  science  \\\n",
       "0      0.00   0.0           0.0   0.00             0.00      0.0   \n",
       "1      0.00   0.0           0.0   0.00             0.00      0.0   \n",
       "2      0.06   0.0           0.0   0.13             0.06      0.0   \n",
       "\n",
       "   janet-studying  study  \n",
       "0            0.00    0.1  \n",
       "1            0.00    0.0  \n",
       "2            0.06    0.0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term frequency, inverse document frequency matrix\n",
    "tfidf = tf * idf; tfidf.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tfidf.png)\n",
    "\n",
    "### Normalize vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studying-data</th>\n",
       "      <th>janet-love</th>\n",
       "      <th>science-galvanize</th>\n",
       "      <th>career-field</th>\n",
       "      <th>science-career</th>\n",
       "      <th>study-data</th>\n",
       "      <th>love</th>\n",
       "      <th>love-love</th>\n",
       "      <th>galvanize</th>\n",
       "      <th>career</th>\n",
       "      <th>...</th>\n",
       "      <th>field</th>\n",
       "      <th>love-study</th>\n",
       "      <th>studying</th>\n",
       "      <th>data</th>\n",
       "      <th>data-science</th>\n",
       "      <th>janet</th>\n",
       "      <th>galvanize-janet</th>\n",
       "      <th>science</th>\n",
       "      <th>janet-studying</th>\n",
       "      <th>study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348665</td>\n",
       "      <td>0.386045</td>\n",
       "      <td>0.697329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   studying-data  janet-love  science-galvanize  career-field  science-career  \\\n",
       "0       0.000000    0.000000           0.000000           0.0             0.0   \n",
       "1       0.000000    0.000000           0.000000           0.5             0.5   \n",
       "2       0.287051    0.287051           0.287051           0.0             0.0   \n",
       "\n",
       "   study-data      love  love-love  galvanize  career  ...  field  love-study  \\\n",
       "0    0.348665  0.386045   0.697329   0.000000     0.0  ...    0.0    0.348665   \n",
       "1    0.000000  0.000000   0.000000   0.000000     0.5  ...    0.5    0.000000   \n",
       "2    0.000000  0.105942   0.000000   0.287051     0.0  ...    0.0    0.000000   \n",
       "\n",
       "   studying  data  data-science     janet  galvanize-janet  science  \\\n",
       "0  0.000000   0.0           0.0  0.000000         0.000000      0.0   \n",
       "1  0.000000   0.0           0.0  0.000000         0.000000      0.0   \n",
       "2  0.287051   0.0           0.0  0.574101         0.287051      0.0   \n",
       "\n",
       "   janet-studying     study  \n",
       "0        0.000000  0.348665  \n",
       "1        0.000000  0.000000  \n",
       "2        0.287051  0.000000  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = tfidf / np.linalg.norm(tfidf, axis=1, ord=2).reshape(-1, 1)\n",
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another quick example: Bitcoin tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Give me 12,000 #bitcoin',\n",
    "    'The supply of gold is infinite in our galaxy, the supply of #Bitcoin will always be fixed. Long-term, there will be no comparison between these assets. Gold will be to bitcoin in the future like sand is to gold today.',\n",
    "    'Someone inquired about my #bitcoin exit strategy. I said the exit occurred in the front end. It is a fiat exit.',   \n",
    "    'Give ME 11,000!!!! #bitcoin'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_tokens(doc,ngs=1):\n",
    "    # lower case all words\n",
    "    doc = doc.lower()\n",
    "    # remove accents\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "    # remove odd punctuation\n",
    "    doc = re.sub(r'[~*.^]*!#', '', doc)\n",
    "    # turn document into a list of tokens\n",
    "    doc = word_tokenize(doc)\n",
    "    # removing stopwords (sw) and punctation (pt)\n",
    "    doc = [token for token in doc if token not in sw and token not in pt]\n",
    "    # stem all tokens\n",
    "    doc = [stemmer.stem(token) for token in doc]\n",
    "    # here we are setting up the bigrams if specified\n",
    "    ng = list(map(lambda tup: '-'.join(tup), ngrams(doc, ngs)))\n",
    "    return doc + ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary and initializing the BoW matrix\n",
    "vocabulary = set()\n",
    "for doc in corpus:\n",
    "    for token in doc_to_tokens(doc):\n",
    "        vocabulary.add(token)\n",
    "\n",
    "vocabulary_lookup = {word: i for i, word in enumerate(vocabulary)}\n",
    "matrix = np.zeros((len(corpus), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bag of words\n",
    "for doc_id, doc in enumerate(corpus):\n",
    "    for token in doc_to_tokens(doc):\n",
    "        word_id = vocabulary_lookup[token]\n",
    "        matrix[doc_id][word_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12,000</th>\n",
       "      <th>infinit</th>\n",
       "      <th>strategi</th>\n",
       "      <th>someon</th>\n",
       "      <th>fiat</th>\n",
       "      <th>give</th>\n",
       "      <th>alway</th>\n",
       "      <th>fix</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>today</th>\n",
       "      <th>...</th>\n",
       "      <th>suppli</th>\n",
       "      <th>long-term</th>\n",
       "      <th>comparison</th>\n",
       "      <th>futur</th>\n",
       "      <th>said</th>\n",
       "      <th>11,000</th>\n",
       "      <th>galaxi</th>\n",
       "      <th>end</th>\n",
       "      <th>front</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   12,000  infinit  strategi  someon  fiat  give  alway  fix  bitcoin  today  \\\n",
       "0       2        0         0       0     0     2      0    0        2      0   \n",
       "1       0        2         0       0     0     0      2    2        4      2   \n",
       "2       0        0         2       2     2     0      0    0        2      0   \n",
       "3       0        0         0       0     0     2      0    0        2      0   \n",
       "\n",
       "   ...  suppli  long-term  comparison  futur  said  11,000  galaxi  end  \\\n",
       "0  ...       0          0           0      0     0       0       0    0   \n",
       "1  ...       4          2           2      2     0       0       2    0   \n",
       "2  ...       0          0           0      0     2       0       0    2   \n",
       "3  ...       0          0           0      0     0       2       0    0   \n",
       "\n",
       "   front  asset  \n",
       "0      0      0  \n",
       "1      0      2  \n",
       "2      2      0  \n",
       "3      0      0  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizing the bag of words\n",
    "columns = sorted(vocabulary_lookup, key=lambda key: vocabulary_lookup[key])\n",
    "df = pd.DataFrame(matrix.astype('int'), columns=columns); df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12,000</th>\n",
       "      <th>infinit</th>\n",
       "      <th>strategi</th>\n",
       "      <th>someon</th>\n",
       "      <th>fiat</th>\n",
       "      <th>give</th>\n",
       "      <th>alway</th>\n",
       "      <th>fix</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>today</th>\n",
       "      <th>...</th>\n",
       "      <th>suppli</th>\n",
       "      <th>long-term</th>\n",
       "      <th>comparison</th>\n",
       "      <th>futur</th>\n",
       "      <th>said</th>\n",
       "      <th>11,000</th>\n",
       "      <th>galaxi</th>\n",
       "      <th>end</th>\n",
       "      <th>front</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   12,000  infinit  strategi  someon  fiat  give  alway   fix  bitcoin  today  \\\n",
       "0    0.46     0.00      0.00    0.00  0.00  0.23   0.00  0.00      0.0   0.00   \n",
       "1    0.00     0.08      0.00    0.00  0.00  0.00   0.08  0.08      0.0   0.08   \n",
       "2    0.00     0.00      0.12    0.12  0.12  0.00   0.00  0.00      0.0   0.00   \n",
       "3    0.00     0.00      0.00    0.00  0.00  0.23   0.00  0.00      0.0   0.00   \n",
       "\n",
       "   ...  suppli  long-term  comparison  futur  said  11,000  galaxi   end  \\\n",
       "0  ...    0.00       0.00        0.00   0.00  0.00    0.00    0.00  0.00   \n",
       "1  ...    0.15       0.08        0.08   0.08  0.00    0.00    0.08  0.00   \n",
       "2  ...    0.00       0.00        0.00   0.00  0.12    0.00    0.00  0.12   \n",
       "3  ...    0.00       0.00        0.00   0.00  0.00    0.46    0.00  0.00   \n",
       "\n",
       "   front  asset  \n",
       "0   0.00   0.00  \n",
       "1   0.00   0.08  \n",
       "2   0.12   0.00  \n",
       "3   0.00   0.00  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dividing each document vector by the sum of its counts, producing a term frequency vector\n",
    "tf = df / df.sum(axis=1).values.reshape(-1, 1)\n",
    "# computing the inverse document frequency\n",
    "idf = np.log(matrix.shape[0] / np.sum(matrix > 0, axis=0))\n",
    "# term frequency, inverse document frequency matrix\n",
    "tfidf = tf * idf; tfidf.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Normalize Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12,000</th>\n",
       "      <th>infinit</th>\n",
       "      <th>strategi</th>\n",
       "      <th>someon</th>\n",
       "      <th>fiat</th>\n",
       "      <th>give</th>\n",
       "      <th>alway</th>\n",
       "      <th>fix</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>today</th>\n",
       "      <th>...</th>\n",
       "      <th>suppli</th>\n",
       "      <th>long-term</th>\n",
       "      <th>comparison</th>\n",
       "      <th>futur</th>\n",
       "      <th>said</th>\n",
       "      <th>11,000</th>\n",
       "      <th>galaxi</th>\n",
       "      <th>end</th>\n",
       "      <th>front</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   12,000  infinit  strategi  someon  fiat  give  alway  fix  bitcoin  today  \\\n",
       "0    0.89      0.0      0.00    0.00  0.00  0.45    0.0  0.0      0.0    0.0   \n",
       "1    0.00      0.2      0.00    0.00  0.00  0.00    0.2  0.2      0.0    0.2   \n",
       "2    0.00      0.0      0.24    0.24  0.24  0.00    0.0  0.0      0.0    0.0   \n",
       "3    0.00      0.0      0.00    0.00  0.00  0.45    0.0  0.0      0.0    0.0   \n",
       "\n",
       "   ...  suppli  long-term  comparison  futur  said  11,000  galaxi   end  \\\n",
       "0  ...    0.00        0.0         0.0    0.0  0.00    0.00     0.0  0.00   \n",
       "1  ...    0.41        0.2         0.2    0.2  0.00    0.00     0.2  0.00   \n",
       "2  ...    0.00        0.0         0.0    0.0  0.24    0.00     0.0  0.24   \n",
       "3  ...    0.00        0.0         0.0    0.0  0.00    0.89     0.0  0.00   \n",
       "\n",
       "   front  asset  \n",
       "0   0.00    0.0  \n",
       "1   0.00    0.2  \n",
       "2   0.24    0.0  \n",
       "3   0.00    0.0  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting each tfidf vector to be a unit vector\n",
    "normalized = tfidf / np.linalg.norm(tfidf, axis=1, ord=2).reshape(-1, 1); normalized.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all vectors have length 1\n",
    "np.linalg.norm(normalized, axis=1, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing two documents / Similarity Measures\n",
    "\n",
    "## Euclidean distance\n",
    "\n",
    "We could try the Euclidean distance $||\\vec{x}-\\vec{y}||$  \n",
    "\n",
    "The euclidean distance goes up with the length of a document. Intuitively, duplicating each word in our bag of words generates a vector that points in exactly the same direction, however, the euclidean distance goes up. One solution is to normalize vectors before calculating the euclidean distance. Now increasing the length of a document does not change the Euclidean distance unless the direction of the term frequency vector changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12,000</th>\n",
       "      <th>infinit</th>\n",
       "      <th>strategi</th>\n",
       "      <th>someon</th>\n",
       "      <th>fiat</th>\n",
       "      <th>give</th>\n",
       "      <th>alway</th>\n",
       "      <th>fix</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>today</th>\n",
       "      <th>...</th>\n",
       "      <th>suppli</th>\n",
       "      <th>long-term</th>\n",
       "      <th>comparison</th>\n",
       "      <th>futur</th>\n",
       "      <th>said</th>\n",
       "      <th>11,000</th>\n",
       "      <th>galaxi</th>\n",
       "      <th>end</th>\n",
       "      <th>front</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     12,000   infinit  strategi    someon      fiat      give     alway  \\\n",
       "0  0.894427  0.000000  0.000000  0.000000  0.000000  0.447214  0.000000   \n",
       "1  0.000000  0.204124  0.000000  0.000000  0.000000  0.000000  0.204124   \n",
       "2  0.000000  0.000000  0.242536  0.242536  0.242536  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.447214  0.000000   \n",
       "\n",
       "        fix  bitcoin     today  ...    suppli  long-term  comparison  \\\n",
       "0  0.000000      0.0  0.000000  ...  0.000000   0.000000    0.000000   \n",
       "1  0.204124      0.0  0.204124  ...  0.408248   0.204124    0.204124   \n",
       "2  0.000000      0.0  0.000000  ...  0.000000   0.000000    0.000000   \n",
       "3  0.000000      0.0  0.000000  ...  0.000000   0.000000    0.000000   \n",
       "\n",
       "      futur      said    11,000    galaxi       end     front     asset  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.204124  0.000000  0.000000  0.204124  0.000000  0.000000  0.204124  \n",
       "2  0.000000  0.242536  0.000000  0.000000  0.242536  0.242536  0.000000  \n",
       "3  0.000000  0.000000  0.894427  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized #.iloc[3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance between tweet 0 and 1 is 1.4142135623730951\n",
      "Euclidean Distance between tweet 0 and 2 is 1.4142135623730951\n",
      "Euclidean Distance between tweet 0 and 3 is 1.2649110640673518\n",
      "Euclidean Distance between tweet 1 and 2 is 1.4142135623730951\n",
      "Euclidean Distance between tweet 1 and 3 is 1.4142135623730951\n",
      "Euclidean Distance between tweet 2 and 3 is 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(i+1,4):\n",
    "        print('Euclidean Distance between tweet {} and {} is {}'.format(i,j, \n",
    "                np.linalg.norm(normalized.iloc[i].values - normalized.iloc[j].values)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cosine Similarity\n",
    "Recall that for two vector $\\vec{x}$ and $\\vec{y}$ that $\\vec{x} \\cdot \\vec{y} = ||\\vec{x}|| ||\\vec{y}|| \\cos{\\theta}$. And so,\n",
    "\n",
    "$\\frac{\\vec{x} \\cdot \\vec{y} }{||\\vec{x}|| ||\\vec{y}||} = \\cos{\\theta}$\n",
    "\n",
    "$\\theta$ can only range from 0 to 90 degrees, because tf-idf vectors are non-negative. Therefore cos $\\theta$ ranges from 0 to 1. Documents that are exactly identical will have $\\cos{\\theta} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between tweet 0 and 1 is 0.0\n",
      "Cosine Similarity between tweet 0 and 2 is 0.0\n",
      "Cosine Similarity between tweet 0 and 3 is 0.19999999999999998\n",
      "Cosine Similarity between tweet 1 and 2 is 0.0\n",
      "Cosine Similarity between tweet 1 and 3 is 0.0\n",
      "Cosine Similarity between tweet 2 and 3 is 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(i+1,4):\n",
    "         print('Cosine Similarity between tweet {} and {} is {}'.format(i,j, \n",
    "                np.dot(normalized.iloc[i].values, normalized.iloc[j].values)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet 0 and 3 seem to be the most similar let's remember what they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Give me 12,000 #bitcoin', 'Give ME 11,000!!!! #bitcoin')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0],corpus[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Libraries \n",
    "\n",
    "## NLTK \n",
    "\n",
    "Above I have used NLTK, which is one of the popular NLP libraries.  It is a good NLP library for learning which is why I have used it above. However, there are more efficient NLP libraries.\n",
    "\n",
    "## Sklearn \n",
    "\n",
    "Sklearn is great and easy to use! Its good for a first try with NLP. However, if you want more fine-grain control use NLTK or Spacy.  \n",
    "\n",
    "### Demo \n",
    "\n",
    "*This demo is based on Sklearns demo in their docs.*\n",
    "\n",
    "Sklearn module which are helpful for NLP.  The first is the Count Vectorizer which takes a collection of text and converts it to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['document', 'first', 'one', 'second', 'third'],\n",
       " array([[1, 1, 0, 0, 0],\n",
       "        [2, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 0, 0]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(  ) # stop_words= ['this', 'is', 'the', 'and' ] ) # stop_words ='english') # \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names(), X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's the difference between fit, transform and fit_transform?  \n",
    "- Why would this matter for us when using machine learning? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'and this',\n",
       " 'document',\n",
       " 'document is',\n",
       " 'first',\n",
       " 'first document',\n",
       " 'is',\n",
       " 'is the',\n",
       " 'is this',\n",
       " 'one',\n",
       " 'second',\n",
       " 'second document',\n",
       " 'the',\n",
       " 'the first',\n",
       " 'the second',\n",
       " 'the third',\n",
       " 'third',\n",
       " 'third one',\n",
       " 'this',\n",
       " 'this document',\n",
       " 'this is',\n",
       " 'this the']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(ngram_range=(1, 2)) # word specifies n-grams should be created with words\n",
    "vectorizer2.fit_transform(corpus)\n",
    "vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorizer and Transfomer\n",
    "\n",
    "The `TfidfTransformer` converts a count vectorized document into a normalized TF or TFIDF vector.\n",
    "\n",
    "TFIDF Vectorizer in sklearn is `CountVectorizer` followed by `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.         0.38408524 0.\n",
      "  0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762 0.28108867 0.\n",
      "  0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.         0.26710379 0.51184851\n",
      "  0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.         0.38408524 0.\n",
      "  0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'the', 'this']\n",
      "[[0.46979139 0.58028582 0.38408524 0.38408524 0.38408524]\n",
      " [0.81614027 0.         0.33362407 0.33362407 0.33362407]\n",
      " [0.         0.         0.57735027 0.57735027 0.57735027]\n",
      " [0.46979139 0.58028582 0.38408524 0.38408524 0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5)  # maximum number of features\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'one', 'second', 'third']\n",
      "[[0.         0.62922751 0.77722116 0.         0.         0.        ]\n",
      " [0.         0.78722298 0.         0.         0.61666846 0.        ]\n",
      " [0.57735027 0.         0.         0.57735027 0.         0.57735027]\n",
      " [0.         0.62922751 0.77722116 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=3)    # ignore terms with doc freq higher than value \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'the', 'this']\n",
      "[[0.46979139 0.58028582 0.38408524 0.38408524 0.38408524]\n",
      " [0.81614027 0.         0.33362407 0.33362407 0.33362407]\n",
      " [0.         0.         0.57735027 0.57735027 0.57735027]\n",
      " [0.46979139 0.58028582 0.38408524 0.38408524 0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2)     # ignore terms with doc freq lower than value; can do counts or percent\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy \n",
    "\n",
    "[Spacy](https://spacy.io/) is a NLP library with similar functionality to NLTK but more efficient and thus better for production environments.  We suggest looking into Spacy if you are doing a NLP capstone project. \n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png' width=400 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "1. What is a corpus? \n",
    "2. Given a corpus of tweets, how do I convert those tweets into a bag of words? \n",
    "3. How do your vectorize the bag of words? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Content \n",
    "\n",
    "## 1.5. Part-of-Speech tagging\n",
    "\n",
    "This is an alternative process that relies on machine learning to tag each word in a sentence with its function. In libraries such as NLTK, there are embedded tools to do that. Tags detected depend on the corpus used for training. In NLTK, the function `nltk.pos_tag()` uses the [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "### nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- sentence tags: [('My', 'PRP$'), ('caring', 'VBG'), ('mother', 'NN'), ('drove', 'VBD'), ('me', 'PRP'), ('to', 'TO'), ('the', 'DT'), ('airport', 'NN'), ('with', 'IN'), ('the', 'DT'), ('windows', 'NNS'), ('rolled', 'VBD'), ('down', 'RB'), ('.', '.')]\n",
      "--- sentence tags: [('It', 'PRP'), ('was', 'VBD'), ('seventy-five', 'JJ'), ('degrees', 'NNS'), ('in', 'IN'), ('Phoenix', 'NNP'), (',', ','), ('the', 'DT'), ('sky', 'NN'), ('a', 'DT'), ('perfect', 'JJ'), (',', ','), ('cloudless', 'JJ'), ('blue', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Here I have some language where I have already tokenized the language\n",
    "tokens = [['My','caring','mother','drove','me', 'to', 'the', 'airport', 'with', 'the','windows','rolled', 'down', '.'],\n",
    " ['It','was','seventy-five','degrees','in', 'Phoenix', ',', 'the', 'sky', 'a',  'perfect',',', 'cloudless',  'blue', '.']]\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "sent_tags = list(map(pos_tag, tokens))\n",
    "\n",
    "for sent in sent_tags:\n",
    "    print(\"--- sentence tags: {}\".format(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter verbs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- verbs:\n",
      "[('caring', 'VBG'), ('drove', 'VBD'), ('rolled', 'VBD')]\n",
      "--- verbs:\n",
      "[('was', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tags:\n",
    "    tags_filtered = [t for t in sent if t[1].startswith('VB')]\n",
    "    print(\"--- verbs:\\n{}\".format(tags_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
