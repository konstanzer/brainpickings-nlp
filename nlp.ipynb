{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import enchant\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/articles.csv', index_col='index')\n",
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "art = []\n",
    "for ix, tag in enumerate(df.tags):\n",
    "    if re.match(\".* art,.*\", tag) or re.match(\"art,.*\", tag): art.append(ix)\n",
    "\n",
    "science = []\n",
    "for ix, tag in enumerate(df.tags):\n",
    "    if re.match(\".*science,.*\", tag): science.append(ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "BrainPickings.org has 5000+ articles containing 5 million words. The author is interested in the evolving themes in her work. While this can be visualized with a stacked barchart (years on x, count on y, stacking is based on tags), I want to turn this into an NLP problem involving vectorization of the articles and then classification. The classification can be done in two ways: time and tags. Time would predict Early (written before 2014) or Late (written 2014-present) while tags would divide articles by popular tags and then try to guess which tag the article is filed under. Tags should be different, like Science an Art, two popular ones. In order to use rarer tags, you will have to undersample the more popular class \n",
    "\n",
    "Classifying articles into one of many tage is a multi-label problem. You'll want to calculate probabilities for each tag for every document. When making the model, think about the target and how to best transform the \"natural language\" into a vector that captures the essence of the target. Suppose you want to classify an article as being about Art or Science. A basic model would be to look for the words \"art\" and \"science\" in the article and return a vector of the counts. So if an article says \"art\" once and \"science\" three times th vector is [1,3]. You want the simplest representation of the article that gives the clearest signal of what the classification should be.\n",
    "\n",
    "If the classification is about whether the author wrote the article early in their career or later, then you'll want to represent the article as a different vector. Maybe the inexperinced writer used a lot of curse words or said \"like\" to much. Maybe the stronger writer writes longer sentences or uses a wider vocabulary. In this case, using too small a vocabulary will not be a good idea. You want to represent the early articles as sparser with more reliance on commoner words. \n",
    "\n",
    "In choosing the right vector to capture the right signal, you'll want to think about the particular problem. Do you need rare words, do you need to filter out words that occur in most articles, do you need stems or lemmas, do you need puncutation, do you want a few stop words or many? Be careful as well you don't make a useless model. Suppose you left t4he date in the article and the model just used that to predict when it was written. Or maybe the author always signed off with a phrase and then stopped doing it, so the model just sees that.\n",
    "\n",
    "Topic modeling is the process of discovering salient feautures in a corpus in an unsupervied way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_df=df.iloc[art]\n",
    "art_df[\"art\"]=1\n",
    "sci_df=df.iloc[science]\n",
    "sci_df[\"art\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([art_df, sci_df])\n",
    "#~250 articles are duplicates, tagged both art and science\n",
    "df2=df2[df2.index.value_counts()==1]\n",
    "df2=df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the corpus...\n",
    "documents = df2.title + df2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "documents = documents.str.lower()\n",
    "documents = documents.str.replace(',','')\n",
    "documents = documents.str.replace('[^\\w\\s]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5  mostly  vintage children s books by iconic graphic designers   saul bass milton glaser paula scher bruno munari paul rand as a lover of  children s books  i have a particularly soft spot for little known gems by well known creators  after  two   rounds  of excavating obscure children s books by famous authors of literature for grown ups and icons of the  art   world  here are five wonderful vintage children s books by some of history s most celebrated graphic designers  henri s walk to paris by saul bass saul bass   1920 1996  is commonly considered the  greatest graphic designer  of all time responsible for some of the most timeless logos and most memorable  film title sequences  of the twentieth century  in 1962 bass collaborated with former librarian  leonore klein  on his only children s book which spent decades as a prized out of print collector s item  this year half a century later rizzoli reprinted  henri s walk to paris    public library     an absolute gem like only bass can deliver at once boldly minimalist and incredibly rich telling the sweet aspirational colorful story of a boy who lives in rural france and dreams of going to paris  originally featured here in february with  more images   the alphazeds by milton glaser many of us regard  milton glaser  as the greatest graphic designer alive  from the iconic i   ny logo to his prolific newspaper and magazine designs logos brand identities posters and other celebrated visual ephemera his work seeks  to inform and delight   in 2003 he collaborated with his wife shirley glaser on  the alphazeds    public library     mighty fuel for my obsession with  alphabet books  in which the letters of the alphabet turn into a boisterous bunch and meet one another for the first time in a small yellow room  delightful havoc ensues  in 2005 the duo collaborated once again producing  the rabbit race    public library     an adaptation of the famous  aesop fable   the brownstone by paula scher paula scher  might be best known for her  iconic identity design  and most recently her  obsessive typographic maps  but in 1973 she teamed up with pioneering documentary style cartoonist  stanley mack  to produce  the brownstone   public library    it tells the tale of six animal families who have trouble finding the right apartment in a classic new york city brownstone building  the collaboration is somewhat surprising   scher a formidable visual communicator herself took the role of writer while mack illustrated the story  the book is long out of print but you can find a used copy at reasonable price with some rummaging online or look for it in your favorite public library  the elephant s wish by bruno munari italian creative polymath  bruno munari  has tried his hand with celebrated success at painting sculpture film industrial design graphic design and literature  in 1945 he expanded his roster of talents into children s books with  the elephant s wish    public library     a stunningly illustrated story with playful folding flaps to add tactile delight  it was published in the u s  in 1959  this book is also out of print  hey rizzoli what are you waiting for   but  used copies  are reasonably priced and you can of course look for one in your local library  in 1960 munari followed up with the graphically astonishing  abc    public library   then with  zoo   public library  in 1963  the two were reissued in 2006 and 2005 respectively  images courtesy of  douglas stewart fine books sparkle and spin by paul rand in the late 1950s legendary graphic designer and  notorious curmudgeon   paul rand  and his then wife anne set out to write and illustrate a series of children s books  first came  sparkle and spin  a book about words    public library   originally featured here  in february   with its bold playful interplay of words and pictures the book encourages an understanding of the relationship between language and image shape and sound thought and expression a lens we ve also seen when italian novelist and philosopher umberto eco  introduced young readers to semiotics  in the same period  though the cover of the 2006 reprint with its all too literal glitter gimmick would have likely sent rand into a vapid fury the book is an absolute treasure one i m happy to see survive the out of print fate of all too many mid century gems  sparkle and spin  was followed by  little 1  in 1962 and the out of print incredibly hard to find  listen  listen   in 1970  for more seminal vintage children s book illustration see the fantastic  children s picturebooks  the art of visual storytelling   '"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2766x25612 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 796009 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF = TfidfVectorizer(strip_accents=\"unicode\",\n",
    "                             min_df=3,\n",
    "                             max_df=.96,\n",
    "                             stop_words='english') #ngram_range=(1,2)\n",
    "X = TFIDF.fit_transform(documents)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25612"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = np.array(list(TFIDF.vocabulary_.keys()))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mmm', 'invisible', 'furniture', ..., 'lisel', 'mueller',\n",
       "       'conservancy'], dtype='<U19')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.02513346, 0.        , 0.03838287, ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.todense()[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1827\n",
       "0     939\n",
       "Name: art, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df2.art\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    stratify=y, random_state=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2212, 14210)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversample science class\n",
    "X_train, y_train = SMOTE().fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2922, 14210)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
